# Classifier using Neural Network

```{r}
library(tidyverse)
```

## Dataset

Using the classic Iris Dataset

```{r}
df <- as_tibble(iris)
str(df)
```

```{r}
head(df)
```

## Basic EDA

Prepare three dataframes

- df contains information for each flower
- df_long contains information for each measurement
- df_wide contains information for each part

```{r}
df_long <- df %>% 
    gather(key = Part_Measure, value = Value, -Species) %>% #Gather columns
    separate(col = Part_Measure, into = c("Part", "Measure"), sep = "\\.") #Break gathered column into two columns
df_long$Measure <- as.factor(df_long$Measure)
head(df_long)
```

```{r}
df$FlowerID <- 1:nrow(df)
df_wide <- df %>% 
    gather(key = Part_Measure, value = Value, -Species, -FlowerID) %>% 
    separate(col = Part_Measure, into = c("Part", "Measure"), sep = "\\.") %>% 
    spread(key = Measure, value = Value)
head(df_wide)
```

Q: How do measurements compare between species?

```{r}
ggplot(df_long, aes(x = Species, y = Value, col = Part)) +
    geom_jitter()+
    facet_grid(. ~ Measure)
```

Right away, we can see that the Setosa is distinct from the other two species, especially by Petal.
The sepal measurements between the Versicolor and Virginica species appear similar, especially width
Q: How does length and width of each part relate for each species?


```{r}
ggplot(df_wide, aes(x = Length, y = Width, col = Part)) +
    geom_jitter() +
    facet_grid(. ~ Species)
```

Q: How do the species compare to each other?
```{r}
ggplot(df_wide, aes(x=Length, y = Width, col = Species)) +
    geom_jitter() +
    facet_grid(. ~ Part)
```

Petal measurements are distinct between each species.
Sepal widths of Virginica and Versicolor overlap but values of length vary.

Wonderings:
- can a simple neural network be trained to distinguish flower species?

## Building the Model

```{r}
library(R6)
```

First lets define the functions to be used in the neural network. I am using a sigmoid activation for the neurons, and softmax for the output layer. My cost function I decided to use cross entropy. 
Decision was motivated by results found when searching online for activation functions for non-binary classification.

http://neuralnetworksanddeeplearning.com/chap3.html?source=post_page---------------------------#introducing_the_cross-entropy_cost_function

```{r}
sigmoid <- function(x) {
    1/(1+exp(-x))
}

d_sigmoid <- function(x) {
    sigmoid(x)*(1-sigmoid(x))
}

softmax <- function(x) {
    exps <- exp(x-max(x))
    exps/sum(exps)
}

cross_entropy <- function(pred, real) {
    n_samples <- length(real)
    residuals <- pred - real
    residuals/n_samples
}
```

Time to build the framework for a neural network object. 

Because part of the intention of this workbook is to document and example my work/thought process, I will include the code I write to test functionality before presenting the final product. 
```{r}
NeuralNetwork <- R6Class("NeuralNetwork",
    public = list(
        X = NULL, Y = NULL,
        W1 = NULL, B1 = NULL,
        W2 = NULL, B2 = NULL,
        Lr = NULL, Neurons = NULL,
        A1= NULL,
        output = NULL,
        initialize = function(data, real, neurons = 5, rate = 0.0001) {
            self$X <- data
            self$Neurons <- neurons
            self$Lr <- rate
            ip_dim = dim(data)[2]
            op_dim = dim(real)[2]
            
            self$W1 <- matrix(rnorm(ip_dim*self$Neurons), ip_dim, self$Neurons)
            self$W2 <- matrix(rnorm(self$Neurons*op_dim), self$Neurons, op_dim)
            self$B1 <- matrix(0, 1, self$Neurons)
            self$B2 <- matrix(0, 1, op_dim)
            self$Y <- real
        },
        feedforward = function() {
            self$A1 <- "works"
        },
        backprop = function() {
            
        },
        predict = function() {
            
        }
    )
)
```

Prepare test input matrices. 

- The final model will not be trained with the entire dataset. 
- After proof of concept I will randomly divide the data the into testing and training.
```{r}
test_data <- data.matrix(df[,c(1:4)])
for (level in levels(df$Species)) {
    df[[level]] <- ifelse(df$Species == level, 1, 0)
}
test_real <- data.matrix(df[,levels(df$Species)])
```

Making sure that I can load the data into a new neural network object.
```{r}
myNN <- NeuralNetwork$new(data = test_data, real = test_real)
str(myNN)
```
Before writing the methods for feedforward, backpropogation, I want to practice writing out their functionality in R.

Lets grab some variables
```{r}
#B1 <- myNN$B1
#B2 <- myNN$B2
B1 <- matrix(rnorm(5),1,5) #1x5 matrix
B2 <- matrix(rnorm(3),1,3) #1x3 matrix
W1 <- myNN$W1 #4x5 matrix
W2 <- myNN$W2 #5x3 matrix
X <- myNN$X #150x4 matrix
Y <- myNN$Y #150x3 matrix
```
Step through a single iteration of feed forward

```{r}
#For inputting 1 row of x
feedforward <- function(x, w1, w2, b1, b2) {
    z1 <- x %*% w1 + b1
    h <- sigmoid(z1)
    z2 <- h %*% w2 + b2
    list(output = softmax(z2), h = h)
}
#For inputting all rows of X
feedforward2 <- function(x, w1, w2, b1, b2) {
    z1 <- sweep(x %*% w1,2,b1,FUN="+")  #Had to use sweep to add b1 to each row of x %*% w1
    h <- t(apply(z1, 1, sigmoid))
    z2 <- sweep(h %*% w2,2,b2,FUN="+")
    list(output = t(apply(z2,1,softmax)))
}
```

Testing to make sure that softmax is returning values that make sense.
```{r}
test_output <- feedforward2(X,W1,W2,B1,B2)
head(test_output$output)
```

For determining selection verses actual
```{r}
predition <- as.matrix(apply(test_output$output, 1, which.max))
actual <- as.matrix(apply(test_real), 1, which.max)
```

```{r}
Z1 <- sweep(X %*% W1,2,B1,FUN="+")      #Z1 = 150x5 matrix
A1 <- t(apply(Z1, 1, sigmoid))          #A1 = 150x5 matrix
Z2 <- sweep(A1 %*% W2, 2, B2, FUN="+")  #Z2 = 150x3 matrix
A2 <- t(apply(Z2,1,softmax))            #A2 = 150x3 matrix
A1_derv <- d_sigmoid(A1)                #A1_derv = 150x5 matrix
```

Back propogation.
```{r}
Delta_A2 <- (A2 - test_real) #150x3 matrix
Delta_A1 <- (Delta_A2 %*% t(W2)) %*% t(A1_derv)
Errors_Outer <- t(t(Delta_A2) %*% A1)
Errors_Hidden <- 
```















































